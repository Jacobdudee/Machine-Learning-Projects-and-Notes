{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intution Behind Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes formula was influence by Thomas Bayes, a priest who was trying to prove the existence of the christian God. \n",
    "\n",
    "The idea behind Naive Bayes algorithms is to use the information obtained from testing (\"fitting\" the algorithm) into the algorithm after intial fitting, essentially a feedback loop. This is possible due to the \"naive\" assumption that each feature is independant from each other. \n",
    "\n",
    "The Gaussian Naive Bayes algorithm, used for classification, is what is explored here. In sci-kit learn, this function of updating the algorithm is done via the .partial_fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "\n",
    "- Spam filtering\n",
    "- Text prediction\n",
    "- Document Classification\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- Need relatively small sample size (easy to implement)\n",
    "- Extremely fast learning compared to other ML methods\n",
    "- \"The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\" (Sci-kit learn documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsides\n",
    "\n",
    "- bad estimator, probability estimates can not be considered accurate\n",
    "- Limited applications\n",
    "- Can break, phrases do not work well, as only take into account frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varieties\n",
    "\n",
    "- GaussianNB: best for classification\n",
    "- MultinomialNB: \"implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification\"\n",
    "- BernoulliNB: \"implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Naive?\n",
    "\n",
    "In a word probability example, the word order is ignored. So the order of the features is ignored, using multiplication of word frequency to calculate probability of each possible class. In other words, the only thing taken into consideration are the frequencies of each piece of evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample Gaussian NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "GaussianNB(priors=None)\n",
    "print(clf.predict([[-0.8, -1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partial_fit() allows for re-calculation of priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "GaussianNB(priors=None)\n",
    "print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NBAccuracy(features_train, labels_train, features_test, labels_test):\n",
    "    \"\"\" compute the accuracy of your Naive Bayes classifier \"\"\"\n",
    "    ### import the sklearn module for GaussianNB\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    ### create classifier\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    ### fit the classifier on the training features and labels\n",
    "    clf.fit(features_train,labels_train)\n",
    "\n",
    "    ### use the trained classifier to predict labels for the test features\n",
    "    pred = clf.predict(features_test)\n",
    "\n",
    "\n",
    "    ### calculate and return the accuracy on the test data\n",
    "    ### this is slightly different than the example, \n",
    "    ### where we just print the accuracy\n",
    "    ### you might need to import an sklearn module\n",
    "    \n",
    "    #Metrics Version\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(pred, labels_test)\n",
    "    \n",
    "    #Built in GaussianNB function\n",
    "    #accuracy = clf.score(features_test,labels_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
