{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If given data of any shape whatesoever, PCA finds new coordinate system and tells you how important they are\n",
    "\n",
    "- by translation and rotation only\n",
    "- moves center of coordinate system to center of data\n",
    "- moves the center of x-axis to center of variation\n",
    "- moves y-axis orthoginal to x-axis\n",
    "\n",
    "### Result: reduces dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurable vs. Latent features\n",
    "\n",
    "- Measurable: vars you can measure\n",
    "\n",
    "- Latent: vars you cant measure directly but drive the feature\n",
    "\n",
    "- Variance (PCA/stats): spread of feature distribution\n",
    "\n",
    "##### Why compress data on direction of largest variance (longer line, x-prime) when y-prime is shorter?\n",
    "- Because it retains the maximum amount of information\n",
    "- Variance between principal component and point determines information lost\n",
    "- So points with greater distance from principal component line will lose more information\n",
    "- Projecting PCA line across the line of largest variance will minimize information loss\n",
    "\n",
    "#### Problem: not scalable\n",
    "- if you have many features, can't scale feature detection individually\n",
    "\n",
    "#### Solution: can put features together and combine into new features and rank feature power\n",
    "- creates multiple principal components, ex: 1. Neighborhood, 2. house size\n",
    "\n",
    "##### issues: hard to interpret but there are takeaways\n",
    "\n",
    "\n",
    "#### The max number of possible principal components is the number of features in the dataset\n",
    "\n",
    "## In SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You create and fit pca similar to other sklearn algorithms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doPCA(data,n_components=2):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(data)#some data here\n",
    "    return pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = doPCA(data)\n",
    "print(pca.explained_variance_ratio_)\n",
    "#explained variance tells you how much % of the data is in each pca "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can grab pca components like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_pc = pca.components_[0]\n",
    "second_pc = pca.components_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get useful information, here is a potential plotting method, and also how to get your original data tranformed with the pca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_data = pca.transform(data)\n",
    "for i, j in zip(transformed_data,data):\n",
    "    plt.scatter(first_pc[0] * i[0], first_pc[1] * i[0], color='r')\n",
    "    plt.scatter(second_pc[0] * i[0], second_pc[1] * i[0], color='c')\n",
    "    plt.scatter(j[0],j[1],color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use PCA\n",
    "\n",
    "- latent features driving patterns in data\n",
    "    - ex.: big shots at Enron\n",
    "    \n",
    "- Dimensionality reduction\n",
    "    - can help visualize high dimension data\n",
    "        - project data down to first 2 pca, and plot\n",
    "        - then plotting algo performance is easier\n",
    "    - reduces noise\n",
    "    - pre-processing before using other algorithm (regression, classification)\n",
    "        - works better because there are fewer inputs \n",
    "        - ex: eigen faces\n",
    "\n",
    "## Example: PCA for Facial Recognition\n",
    "\n",
    "#### What makes PCA good for facial recognition?\n",
    "- pictures of faces have high input dimensionality (many pixels)\n",
    "- faces have general patterns that can be captured in smaller dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to select a good amount of PCs to use?\n",
    "- train on different # of pcs and compare F1 score\n",
    "    - will show point of diminshing returns\n",
    "- don't do feature selection before- PCA can \"salvage\" different features because it combines them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
